"""Murasaki Translator - Production Translation Engine"""
import argparse
import sys
import os
import time
import json
import re
import subprocess
import threading
from contextlib import nullcontext
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, Future, as_completed

from pathlib import Path

# Add middleware directory to sys.path for package imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Force UTF-8 for stdout/stderr (Windows console fix)
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

# Global Lock for thread-safe printing to stdout
stdout_lock = threading.Lock()

def safe_print_json(prefix, data):
    """Thread-safe JSON printing to stdout."""
    with stdout_lock:
        sys.stdout.write(f"\n{prefix}:{json.dumps(data, ensure_ascii=False)}\n")
        sys.stdout.flush()

def safe_print(msg):
    """Thread-safe generic printing."""
    with stdout_lock:
        print(msg)


from murasaki_translator.core.chunker import Chunker
from murasaki_translator.core.prompt import PromptBuilder
from murasaki_translator.core.engine import InferenceEngine
from murasaki_translator.core.parser import ResponseParser
from murasaki_translator.core.quality_checker import QualityChecker, format_warnings_for_log, calculate_glossary_coverage
from murasaki_translator.core.text_protector import TextProtector  # [Experimental] 占位符保护
from murasaki_translator.core.cache import TranslationCache  # 翻译缓存用于校对
from rule_processor import RuleProcessor
from murasaki_translator.utils.monitor import HardwareMonitor
from murasaki_translator.utils.line_aligner import LineAligner
from murasaki_translator.fixer import NumberFixer, Normalizer, PunctuationFixer, KanaFixer, RubyCleaner
from murasaki_translator.documents import DocumentFactory

def load_glossary(path: Optional[str]) -> Dict[str, str]:
    """
    Robustly load glossary from JSON file.
    Supports: 
    - Murasaki Dict: {"jp": "zh"}
    - 通用对象列表: [{"src": "jp", "dst": "zh"}]
    """
    if not path or not os.path.exists(path) or not path.lower().endswith('.json'):
        return {}
    
    try:
        with open(path, 'r', encoding='utf-8-sig') as f:
            data = json.load(f)
        
        # Case 1: Standard Dict format {"src": "dst"}
        if isinstance(data, dict):
            return {str(k): str(v) for k, v in data.items() if k and v}
        
        # Case 2: List of objects
        elif isinstance(data, list):
            glossary = {}
            for idx, entry in enumerate(data):
                if not isinstance(entry, dict): continue
                
                # Heuristic search for source/target keys
                src = entry.get('src') or entry.get('jp') or entry.get('original')
                dst = entry.get('dst') or entry.get('zh') or entry.get('translation')
                
                if src and dst:
                    s, d = str(src), str(dst)
                    if s in glossary:
                         print(f"[Debug] Overwriting glossary entry: {s} (Old: {glossary[s]}, New: {d})")
                    glossary[s] = d
            
            print(f"[Init] Detected List format JSON. Parsed {len(glossary)} valid entries.")
            return glossary
            
        return {}
            
    except Exception as e:
        print(f"[Warning] Failed to load glossary {os.path.basename(path)}: {e}")
        return {}

def load_rules(path: Optional[str]) -> List[Dict]:
    """Load rules from JSON file."""
    if not path or not os.path.exists(path):
        return []
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"[Warning] Failed to load rules: {e}")
        return []


def load_existing_output(output_path: str) -> tuple:
    """
    加载已有输出文件，用于增量翻译。
    返回 (已翻译行数, 已翻译内容列表, 是否有效)
    """
    if not os.path.exists(output_path):
        return 0, [], False
    
    try:
        with open(output_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # 检查是否包含 summary（完整翻译的标志）
        if '=' * 20 in content and 'Translation Summary' in content:
            # 文件已完成，不需要续翻
            return -1, [], False
        
        # 分割为行，过滤空行
        lines = [l for l in content.split('\n') if l.strip()]
        return len(lines), lines, True
    except Exception as e:
        print(f"[Warning] Failed to load existing output: {e}")
        return 0, [], False

def get_missed_terms(source_text: str, translated_text: str, glossary: Dict[str, str]) -> List[tuple]:
    """
    获取原文中出现但译文中未正确翻译的术语列表。
    返回 [(原文术语, 目标译文), ...]
    """
    missed = []
    for src_term, dst_term in glossary.items():
        # 排除单字术语
        if len(src_term) > 1 and src_term in source_text:
            if dst_term not in translated_text:
                missed.append((src_term, dst_term))
    return missed


def build_retry_feedback(missed_terms: List[tuple], coverage: float) -> str:
    """
    构建重试时注入的反馈文本，用于提醒模型注意漏掉的术语。
    """
    if not missed_terms:
        return ""
    
    # 构建术语列表
    terms_str = "、".join([f"「{src}」→「{dst}」" for src, dst in missed_terms[:5]])
    if len(missed_terms) > 5:
        terms_str += f" 等 {len(missed_terms)} 项"
    
    feedback = f"\n\n【系统提示】上一轮翻译中以下术语未正确应用：{terms_str}。请在本次翻译中严格使用术语表中的标准译法，不要擅自简化或省略。"
    
    return feedback


def calculate_skip_blocks(blocks, existing_lines: int) -> int:
    """
    根据已翻译行数计算应该跳过的块数。
    采用保守策略：只跳过完全匹配的块。
    """
    if existing_lines <= 0:
        return 0
    
    cumulative_lines = 0
    for i, block in enumerate(blocks):
        # 估算这个块的输出行数（与输入行数大致相同）
        block_lines = block.prompt_text.count('\n') + 1
        cumulative_lines += block_lines
        
        # 如果累积行数超过已有行数，返回前一个块
        if cumulative_lines >= existing_lines:
            return i  # 从这个块开始重新翻译（保守策略）
    
    return len(blocks)  # 所有块都已完成


def get_gpu_name():
    try:
        # Try finding nvidia-smi
        try:
            # shell=True sometimes helps on Windows if PATH is weird, but usually not needed.
            # Using 'gb18030' to handle Chinese Windows output correctly
            result = subprocess.check_output("nvidia-smi -L", shell=True, stderr=subprocess.STDOUT).decode('gb18030', errors='ignore')
        except:
             return "Unknown / CPU (nvidia-smi failed)"

        names = []
        for line in result.strip().split('\n'):
            if ":" in line and "GPU" in line:
                # Format: GPU 0: NVIDIA GeForce RTX 3090 (UUID: ...)
                # Split by ':' -> ["GPU 0", " NVIDIA GeForce RTX 3090 (UUID", " ...)"]
                parts = line.split(":")
                if len(parts) >= 2:
                    name_part = parts[1].strip()
                    # Remove UUID part if exists
                    if "(" in name_part:
                        name_part = name_part.split("(")[0].strip()
                    names.append(name_part)
        return " & ".join(names) if names else "Unknown GPU"
    except Exception as e:
        return f"Unknown / CPU (Error: {str(e)})"

def format_model_info(model_path: str):
    filename = os.path.basename(model_path)
    
    # Custom Override for Murasaki model
    display_name = filename
    params = "Unknown"
    quant = "Unknown"
    
    if "Murasaki" in filename or "ACGN" in filename or "Step150" in filename:
        display_name = "Murasaki-8B-v0.1"
    
    # Extract details from filename (standard GGUF naming convention: Name-Size-Quant.gguf)
    lower_name = filename.lower()
    
    # Rough parsing
    if "8b" in lower_name: params = "8B"
    elif "72b" in lower_name: params = "72B"
    
    if "q4_k_m" in lower_name: quant = "Q4_K_M"
    elif "q8_0" in lower_name: quant = "Q8_0"
    elif "fp16" in lower_name: quant = "FP16"
    
    return display_name, params, quant


def translate_single_block(args):
    """
    单块翻译模式 - 用于校对界面的重翻功能
    直接翻译 args.single_block 中的文本，支持文本保护，输出 JSON 格式结果
    """
    middleware_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    # Initialize Engine
    engine = InferenceEngine(
        server_path=args.server,
        model_path=args.model,
        gpu_layers=args.gpu_layers,
        ctx_size=args.ctx,
        temperature=args.temperature,
        rep_base=getattr(args, 'rep_penalty_base', 1.0),
        rep_max=getattr(args, 'rep_penalty_max', 1.5),
        no_spawn=getattr(args, 'no_server_spawn', False),
        flash_attn=getattr(args, 'flash_attn', False),
        kv_cache_type=getattr(args, 'kv_cache_type', "q8_0"),
        use_large_batch=getattr(args, 'use_large_batch', False),
        batch_size=getattr(args, 'batch_size', None),
        seed=getattr(args, 'seed', None)
    )
    
    # Load Glossary
    glossary = load_glossary(args.glossary)
    
    # Load Rules (Optional)
    rules_pre = load_rules(args.rules_pre) if hasattr(args, 'rules_pre') and args.rules_pre else []
    rules_post = load_rules(args.rules_post) if hasattr(args, 'rules_post') and args.rules_post else []
    
    pre_processor = RuleProcessor(rules_pre)
    post_processor = RuleProcessor(rules_post)
    
    # Initialize Text Protector
    protector = None
    if getattr(args, 'text_protect', False):
         # Try to get patterns from app data or default
         protector = TextProtector()
    
    try:
        engine.start_server()
        
        # 1. Input Validation
        src_text = args.single_block
        if not src_text or not src_text.strip():
            raise ValueError("Input text is empty")
            
        # 2. Pre-processing & Protection
        processed_src = pre_processor.process(src_text)
        processed_src = Normalizer.normalize(processed_src)
            
        if protector:
            processed_src = protector.protect(processed_src)
            
        # 3. Build Prompt
        messages = prompt_builder.build_messages(
            processed_src,
            enable_cot=args.debug,
            preset=args.preset
        )
        
        # 4. Translate
        raw_output, block_usage = engine.chat_completion(
            messages=messages,
            temperature=args.temperature,
            rep_base=getattr(args, 'rep_penalty_base', 1.0),
            rep_max=getattr(args, 'rep_penalty_max', 1.5),
            block_id=0
        )
        
        if raw_output:
            # 5. Parse & Post-process
            parsed_lines, cot_content = parser.parse(raw_output)
            base_text = '\n'.join(parsed_lines)
            
            # Unified Rule Processor Application (Integrated Restoration)
            # This replaces the hardcoded NumberFixer, PunctuationFixer, KanaFixer calls
            final_dst = post_processor.process(base_text, src_text=src_text, protector=protector)
            
            result = {
                'success': True,
                'src': src_text,
                'dst': final_dst,
                'cot': cot_content if args.debug else ''
            }
        else:
            result = {
                'success': False,
                'src': src_text,
                'dst': '',
                'error': 'Translation failed or empty output'
            }
        
        # Output
        if args.json_output:
            print(f"JSON_RESULT:{json.dumps(result, ensure_ascii=False)}")
        else:
            print(result.get('dst', ''))
            
    except Exception as e:
        if args.json_output:
            print(f"JSON_RESULT:{json.dumps({'success': False, 'error': str(e)}, ensure_ascii=False)}")
        else:
            print(f"Error: {e}")
    finally:
        engine.stop_server()


def main():
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='[%(asctime)s] [%(levelname)s] %(message)s',
        datefmt='%H:%M:%S',
        stream=sys.stdout
    )
    logger = logging.getLogger("murasaki")

    # Argument Parsing
    # Default server path relative to middleware directory
    middleware_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    default_server = os.path.join(middleware_dir, "llama-b7770-bin-win-cuda-12.4-x64", "llama-server.exe")
    default_model = os.path.join(middleware_dir, "models", "ACGN-8B-Step150-Q4_K_M.gguf")
    
    parser = argparse.ArgumentParser(description="Murasaki Translator - High Fidelity System 2 Translation")
    parser.add_argument("--file", required=True, help="Input file path")
    parser.add_argument("--server", default=default_server)
    parser.add_argument("--model", default=default_model)
    parser.add_argument("--glossary", help="Glossary JSON path")
    parser.add_argument("--gpu-layers", type=int, default=-1)
    parser.add_argument("--ctx", type=int, default=8192)
    parser.add_argument("--preset", default="training", choices=["minimal", "training", "short"], help="Prompt preset")
    parser.add_argument("--mode", default="doc", choices=["doc", "line"], help="Translation mode: doc (novel) or line (game/contrast)")
    parser.add_argument("--chunk-size", type=int, default=1000, help="Target char count for doc mode")
    parser.add_argument("--debug", action="store_true", help="Enable CoT stats and timing")
    parser.add_argument("--line-format", default="single", choices=["single", "double"], help="Line spacing format")
    parser.add_argument("--output", help="Custom output file path")
    parser.add_argument("--rules-pre", help="Path to pre-processing rules JSON")
    parser.add_argument("--rules-post", help="Path to post-processing rules JSON")
    parser.add_argument("--save-cot", action="store_true", help="Save CoT debug file")
    parser.add_argument("--save-summary", action="store_true", help="Append summary to output")
    parser.add_argument("--traditional", action="store_true", help="Convert output to Traditional Chinese")
    
    # Experimental Features (可在 GUI 高级设置中开关)
    parser.add_argument("--fix-ruby", action="store_true", help="[Experimental] Clean Ruby annotations from source")
    parser.add_argument("--fix-kana", action="store_true", help="[Experimental] Remove orphan kana from output")
    parser.add_argument("--fix-punctuation", action="store_true", help="[Experimental] Normalize punctuation in output")
    
    # Quality Control Settings (高级质量控制)
    parser.add_argument("--temperature", type=float, default=0.7, help="Model temperature (0.1-1.5, default 0.7)")
    parser.add_argument("--line-check", action="store_true", help="Enable line count validation and auto-retry")
    parser.add_argument("--line-tolerance-abs", type=int, default=10, help="Line count absolute tolerance (default 10)")
    parser.add_argument("--line-tolerance-pct", type=float, default=0.2, help="Line count percent tolerance (default 0.2 = 20%%)")
    parser.add_argument("--rep-penalty-base", type=float, default=1.0, help="Initial repetition penalty (default 1.0)")
    parser.add_argument("--rep-penalty-max", type=float, default=1.5, help="Max repetition penalty (default 1.5)")
    parser.add_argument("--max-retries", type=int, default=3, help="Max retries for empty output (default 3)")
    
    # Glossary Coverage Check (术语表覆盖率检测)
    parser.add_argument("--output-hit-threshold", type=float, default=60.0, help="Min output exact hit percentage to pass (default 60)")
    parser.add_argument("--cot-coverage-threshold", type=float, default=80.0, help="Min CoT coverage percentage to pass (default 80)")
    parser.add_argument("--coverage-retries", type=int, default=3, help="Max retries for low coverage (default 3)")
    
    # Dynamic Retry Strategy (动态重试策略)
    parser.add_argument("--retry-temp-boost", type=float, default=0.2, help="Temperature boost per retry (default 0.2)")
    parser.add_argument("--retry-rep-boost", type=float, default=0.1, help="Repetition penalty boost per retry (default 0.1)")
    parser.add_argument("--retry-prompt-feedback", action="store_true", default=True, help="Inject feedback about missed terms in retry prompts")
    
    # Incremental Translation (增量翻译)
    parser.add_argument("--resume", action="store_true", help="Resume from existing output file (skip translated content)")
    
    # Text Protection (文本保护)
    parser.add_argument("--text-protect", action="store_true", help="Protect variables/tags from translation")
    parser.add_argument("--protect-patterns", help="Path to custom protection patterns file (one regex per line)")
    
    # Cache & Proofreading (缓存与校对)
    parser.add_argument("--save-cache", action="store_true", help="Save translation cache for proofreading")
    parser.add_argument("--cache-path", help="Custom directory to store cache files")
    parser.add_argument("--single-block", help="Translate a single block (for proofreading retranslate)")
    parser.add_argument("--json-output", action="store_true", help="Output result as JSON (for single-block mode)")
    parser.add_argument("--no-server-spawn", action="store_true", help="Client mode: connect to existing server")
    
    parser.add_argument("--concurrency", type=int, default=1, help="Parallel slots count (default 1)")
    # High-Fidelity Granular Settings
    parser.add_argument("--high-fidelity", action="store_true", help="Master Switch: Enable recommended High-Fidelity settings")
    parser.add_argument("--flash-attn", action="store_true", help="Enable Flash Attention")
    parser.add_argument("--kv-cache-type", default="q8_0", choices=["f16", "q8_0", "q5_1", "q4_0"], help="KV Cache Quantization (default: q8_0)")
    parser.add_argument("--use-large-batch", action="store_true", help="Use large batch sizes (b=ub=1024 for safety)")
    parser.add_argument("--batch-size", type=int, help="Manual physical batch size (overrides large-batch default)")
    parser.add_argument("--seed", type=int, help="Lock sampling seed (e.g. 42)")
    
    # Chunk Balancing Strategy
    parser.add_argument("--balance-enable", action="store_true", help="Enable chunk tail balancing")
    parser.add_argument("--balance-threshold", type=float, default=0.6, help="Tail balance threshold (default 0.6)")
    parser.add_argument("--balance-count", type=int, default=3, help="Tail balance range count (default 3)")
    
    args = parser.parse_args()
    
    # High-Fidelity Logic is now handled by the frontend.
    # The backend simply respects the explicit arguments passed for kv_cache, batch size, etc.
    # if args.high_fidelity: ... (Removed)
    
    # Concurrency Hard Limit (New requirement: max 16)
    if args.concurrency > 16:
        print(f"[Warning] Concurrency {args.concurrency} exceeds system limit of 16. Capping to 16.")
        args.concurrency = 16

    # ========================================
    # 单块翻译模式 (用于校对界面重翻)
    # ========================================
    if args.single_block:
        return translate_single_block(args)

    # Path Setup
    input_path = os.path.abspath(args.file)
    if not os.path.exists(input_path):
        print(f"Error: File not found {input_path}")
        return

    # Resolve glossary path before loading
    glossary_path = args.glossary
    if not glossary_path:
        glossary = {}
    else:
        # Try resolving path
        if not os.path.exists(glossary_path):
            # Try finding in glossaries subdirectory relative to script
            script_dir = os.path.dirname(os.path.abspath(__file__))
            candidate = os.path.join(script_dir, 'glossaries', glossary_path)
            if os.path.exists(candidate):
                glossary_path = candidate
            else:
                # Try finding in glossaries subdirectory relative to CWD
                candidate = os.path.join('glossaries', glossary_path)
                if os.path.exists(candidate):
                    glossary_path = candidate
                else:
                    print(f"[Warning] Glossary not found: {glossary_path} (checked absolute, script/glossaries, cwd/glossaries)")
                    glossary_path = None # Indicate no valid glossary path found
        
        if glossary_path:
            glossary = load_glossary(glossary_path)
        else:
            glossary = {}

    # Determine Output Paths
    if args.output:
        output_path = args.output
        base, ext = os.path.splitext(output_path)
        cot_path = f"{base}_cot{ext}"
    else:
        base, ext = os.path.splitext(input_path)
        # Unified naming format v0.1
        suffix = f"_Murasaki-8B-v0.1_{args.preset}_{args.mode}"
        output_path = f"{base}{suffix}{ext}"
        cot_path = f"{base}{suffix}_cot{ext}"

    # Temp Progress Path (Deterministic for Resume)
    temp_progress_path = f"{output_path}.temp.jsonl"
    
    # Clean up OLD temp files in output directory (not glossary dir)
    try:
        out_dir = os.path.dirname(os.path.abspath(output_path))
        for f in os.listdir(out_dir):
            if f.endswith(".temp.jsonl") and f != os.path.basename(temp_progress_path):
                full_p = os.path.join(out_dir, f)
                # Clean files older than 24h
                if time.time() - os.path.getmtime(full_p) > 86400:
                    os.remove(full_p)
    except Exception: pass

    # Load Glossary (Already loaded above)
    print(f"[Init] Loaded glossary: {len(glossary)} entries from {glossary_path or 'None'}")

    # Initialize Components
    # Important: total_ctx = per_slot_ctx * concurrency (handled by engine args if needed)
    # But llama-server -c is total context.
    # The UI passes "ctx" (per slot). So we must multiply here.
    total_ctx = args.ctx * args.concurrency
    
    # Context Hardcap (Llama-3/RoPE limit check)
    MAX_SYSTEM_CTX = 32768
    if total_ctx > MAX_SYSTEM_CTX:
        print(f"[Warning] Requested total context {total_ctx} exceeds safe limit {MAX_SYSTEM_CTX}. Capping.")
        total_ctx = MAX_SYSTEM_CTX

    # Generate Configuration Fingerprint for Resume Integrity
    import hashlib
    config_payload = {
        "chunk_size": args.chunk_size,
        "model": os.path.basename(args.model),
        "concurrency": args.concurrency,
        "preset": args.preset,
        "line_format": args.line_format,
        "ctx": args.ctx,
        "text_protect": args.text_protect,
        "fix_ruby": args.fix_ruby,
        "fix_kana": args.fix_kana,
        "fix_punctuation": args.fix_punctuation,
        "traditional": args.traditional,
        "glossary_path": glossary_path,
        "rules_pre": args.rules_pre,
        "rules_post": args.rules_post,
        "high_fidelity": args.high_fidelity,
        "flash_attn": args.flash_attn,
        "kv_cache_type": args.kv_cache_type,
        "use_large_batch": args.use_large_batch,
        "seed": args.seed,
        "balance_enable": args.balance_enable,
        "balance_threshold": args.balance_threshold,
        "balance_count": args.balance_count,
    }
    config_hash = hashlib.sha256(json.dumps(config_payload, sort_keys=True).encode()).hexdigest()[:16]
    
    # Concurrency Warning (Flops/Bandwidth Bottleneck)
    # Concurrency Warning (Flops/Bandwidth Bottleneck)
    if args.concurrency >= 4:
        print(f"\n[Warning] Concurrency set to {args.concurrency} (>=4). High concurrency may decrease processing speed instead of increasing it.")
        print(f"[Tip] Real speed depends on GPU compute (FLOPs), bandwidth, and parallel processing quantity. If speed drops, try reducing concurrency.\n")

    print(f"Initializing Engine (Server: {args.server}, Parallel: {args.concurrency}, HighFidelity: {args.high_fidelity})...")
    engine = InferenceEngine(
        server_path=args.server, 
        model_path=args.model, 
        n_gpu_layers=args.gpu_layers,
        n_ctx=total_ctx, 
        n_parallel=args.concurrency,
        no_spawn=args.no_server_spawn,
        flash_attn=args.flash_attn,
        kv_cache_type=args.kv_cache_type,
        use_large_batch=args.use_large_batch,
        batch_size=args.batch_size,
        seed=args.seed
    )
    
    chunker = Chunker(
        target_chars=args.chunk_size, 
        max_chars=args.chunk_size * 2, # Soft limit doubled for buffer
        mode=args.mode,
        enable_balance=args.balance_enable,
        balance_threshold=args.balance_threshold,
        balance_range=args.balance_count
    )
    
    prompt_builder = PromptBuilder(glossary)
    response_parser = ResponseParser()
    

    
    # --- Unified Pipeline Injection ---
    def add_unique_rule(rule_list, pattern, r_type='format', pos='append'):
        if any(r.get('pattern') == pattern for r in rule_list): return
        rule_obj = {"type": r_type, "pattern": pattern, "active": True}
        if pos == 'prepend': rule_list.insert(0, rule_obj)
        else: rule_list.append(rule_obj)

    # 1. Pre-rules
    pre_rules = load_rules(args.rules_pre) if args.rules_pre else []
    if args.fix_ruby:
        add_unique_rule(pre_rules, "ruby_cleaner", pos='prepend')
    
    # 2. Post-rules
    post_rules = load_rules(args.rules_post) if args.rules_post else []
    if args.fix_kana:
        add_unique_rule(post_rules, "kana_fixer")
    if args.fix_punctuation:
        add_unique_rule(post_rules, "punctuation_fixer")
    if args.traditional:
        add_unique_rule(post_rules, "traditional_chinese")

    add_unique_rule(post_rules, "number_fixer")

    # [Audit Fix] Auto-detect protection requirement from rules
    protection_rules = [r for r in post_rules if r.get('pattern') == 'restore_protection']
    if protection_rules:
        if not args.text_protect:
            print("[Auto-Config] Detected 'restore_protection' rule. Enabling TextProtector.")
            args.text_protect = True
        
        # Override protector patterns if customPattern is defined in rule options
        # We now support COLLECTING multiple patterns from multiple rules
        custom_patterns = [r.get('options', {}).get('customPattern') for r in protection_rules if r.get('options', {}).get('customPattern')]
        if custom_patterns:
            custom_protector_patterns = custom_patterns
            print(f"[Auto-Config] Using {len(custom_patterns)} custom protection pattern(s) from rules.")
        else:
            custom_protector_patterns = None # Fallback to default in TextProtector
    else:
        custom_protector_patterns = None

    # Inject Line Format Rule
    has_format_rule = any(r.get('pattern', '').startswith('ensure_') for r in post_rules)
    if not has_format_rule:
        if args.line_format == "single":
            add_unique_rule(post_rules, "ensure_single_newline")
        elif args.line_format == "double":
            add_unique_rule(post_rules, "ensure_double_newline")
    
    pre_processor = RuleProcessor(pre_rules)
    post_processor = RuleProcessor(post_rules)

    print(f"Loaded {len(pre_processor.rules)} pre-processing rules.")
    print(f"Loaded {len(post_processor.rules)} post-processing rules.")

    try:
        engine.start_server()
        
        # Read Input using DocumentFactory
        doc = DocumentFactory.get_document(input_path)
        items = doc.load()
            
        # Chunking
        blocks = chunker.process(items)
        print(f"[{args.mode.upper()} Mode] Input split into {len(blocks)} blocks.")
        
        # 源文本统计
        source_lines = len([i for i in items if i['text'].strip()])
        source_chars = sum(len(i['text']) for i in items if i['text'].strip())
        
        # Debug output (only when --debug is enabled)
        if args.debug:
            print(f"[DEBUG] Input lines: {source_lines}, Total chars: {source_chars}")
            for bi, blk in enumerate(blocks):
                print(f"[DEBUG] Block {bi+1}: {len(blk.prompt_text)} chars")
        
        # Streaming Processing
        start_time = time.time() # Ensure initialized early
        total_chars = 0
        total_cot_chars = 0
        total_out_chars = 0  
        total_time = 0
        total_tokens = 0 # Track total tokens used
        total_gen_tokens = 0 # Track total generated tokens (for smooth speed)
        total_gen_time = 0 # Track total generation duration (for smooth speed)
        total_lines = 0 # Track total output lines for stats
        last_progress_time = 0 # For rate limiting
        
        # 初始化翻译缓存（用于校对界面）
        translation_cache = TranslationCache(output_path, custom_cache_dir=args.cache_path) if args.save_cache else None
        
        # Load legacy custom protection patterns file if provided via CLI
        if args.protect_patterns and os.path.exists(args.protect_patterns):
             try:
                 with open(args.protect_patterns, 'r', encoding='utf-8') as f:
                     legacy_patterns = [line.strip() for line in f if line.strip()]
                     # If we don't already have patterns from the new Rule system, use these
                     if not custom_protector_patterns:
                         custom_protector_patterns = legacy_patterns
                         print(f"Loaded {len(custom_protector_patterns)} legacy protection patterns from file.")
                     else:
                         print("[Info] New rule-based protection pattern taking precedence over legacy file.")
             except Exception as e:
                 print(f"[Warning] Failed to load legacy protection patterns: {e}")
        
        gpu_name = get_gpu_name()  # Get GPU Name once
        display_name, params, quant = format_model_info(args.model)
        
        print("\nStarting Translation...")
        print(f"Output: {output_path}")
        if args.save_cot:
            print(f"Debug CoT: {cot_path}")
        print(f"GPU: {gpu_name} (Layers: {args.gpu_layers})")
        print(f"Model: {display_name} ({params}, {quant})")
        
        # 详细配置状态日志
        print("\n[Config] Feature Status:")
        print(f"  Temperature: {args.temperature}")
        print(f"  Line Check: {'[V] Enabled' if args.line_check else '[X] Disabled'} (±{args.line_tolerance_abs}/{args.line_tolerance_pct*100:.0f}%)")
        print(f"  Rep Penalty Retry: Base={args.rep_penalty_base}, Max={args.rep_penalty_max}")
        print(f"  Max Retries: {args.max_retries}")
        print(f"  Glossary Coverage: {'[V] Enabled' if args.output_hit_threshold < 100 else '[X] Disabled'} (Output>={args.output_hit_threshold}% or CoT>={args.cot_coverage_threshold}%, Retries={args.coverage_retries})")
        print(f"  Dynamic Retry: TempBoost={args.retry_temp_boost}, RepBoost={args.retry_rep_boost}, Feedback={'[V]' if args.retry_prompt_feedback else '[X]'}")
        print(f"  Text Protect: {'[V] Enabled' if args.text_protect else '[X] Disabled'}")
        print(f"  Traditional Chinese: {'[V] Enabled' if args.traditional else '[X] Disabled'}")
        print(f"  Resume Mode: {'[V] Enabled' if args.resume else '[X] Disabled'}")
        print(f"  Save Cache: {'[V] Enabled' if args.save_cache else '[X] Disabled'}")
        if args.glossary:
            print(f"  Glossary: {os.path.basename(args.glossary)} ({len(glossary)} entries)")
        else:
            print(f"  Glossary: None")
        print()
        
        # 发送输出路径给前端（用于历史记录和文件打开）
        sys.stdout.write(f"\nJSON_OUTPUT_PATH:{json.dumps({'path': output_path}, ensure_ascii=False)}\n")
        sys.stdout.flush()
        
        # Init Monitor
        monitor = HardwareMonitor()
        if monitor.enabled:
            print(f"Hardware Monitor Active: {monitor.name}")
            
            # Start Monitor Thread
            def run_monitor_loop():
                while True:
                    status = monitor.get_status()
                    if status:
                        safe_print_json("JSON_MONITOR", status)
                    time.sleep(2.0)
            
            monitor_thread = threading.Thread(target=run_monitor_loop, daemon=True)
            monitor_thread.start()
        
        # 定义全局起始时间 (Fix NameError) - MOVED TO TOP
        # start_time = time.time()
        
        # 发送初始脉冲 JSON (Initial Progress Pulse)
        # 这让 GUI 知道总块数，即使任务还未开始处理
        initial_progress = {
            "current": 0,
            "total": len(blocks),
            "percent": 0,
            "total_chars": 0,
            "total_lines": 0,
            "source_chars": source_chars,
            "source_lines": source_lines,
            "speed_chars": 0,
            "elapsed": 0
        }
        safe_print_json("JSON_PROGRESS", initial_progress)
        
        # Incremental Translation / Resume Logic
        skip_blocks_from_output = 0 # Blocks skipped based on final output file
        precalculated_temp = {}    # Blocks already processed and stored in temp file
        existing_content = []      # [Audit Fix] Store lines for document reconstruction
        resume_config_matched = False
        
        if args.resume:
            existing_lines, existing_content, is_valid = load_existing_output(output_path)
            if existing_lines == -1:
                print("[Resume] Output file already complete. Nothing to do.")
                return
            elif is_valid and existing_lines > 0:
                skip_blocks_from_output = calculate_skip_blocks(blocks, existing_lines)
                if skip_blocks_from_output >= len(blocks):
                    print(f"[Resume] All {len(blocks)} blocks already translated. Nothing to do.")
                    return
                print(f"[Resume] Found {existing_lines} existing lines in output. Will skip first {skip_blocks_from_output}/{len(blocks)} blocks.")
            else:
                print("[Resume] No valid existing output found. Starting fresh.")
                existing_content = [] # Reset if invalid

            # Load temporary progress from .temp.jsonl file.
            # Returns: {block_idx: result_dict}
            if os.path.exists(temp_progress_path):
                try:
                    with open(temp_progress_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        if lines:
                            # First line should be config fingerprint
                            try:
                                header = json.loads(lines[0])
                                if header.get("type") == "fingerprint":
                                    if header.get("hash") == config_hash:
                                        resume_config_matched = True
                                        logger.info(f"[Resume] Config fingerprint matched ({config_hash}).")
                                    else:
                                        logger.warning(f"[Resume] Config mismatch! (Saved: {header.get('hash')}, Current: {config_hash}). Restarting.")
                                else:
                                    logger.warning("[Resume] No fingerprint found in temp file. Restarting.")
                            except:
                                logger.warning("[Resume] Invalid fingerprint header. Restarting.")

                            if resume_config_matched:
                                for line in lines[1:]: # Skip fingerprint
                                     data = json.loads(line)
                                     idx = data.get('block_idx')
                                     if idx is not None:
                                         precalculated_temp[idx] = data
                    if precalculated_temp and resume_config_matched:
                         print(f"[Resume] Loaded {len(precalculated_temp)} blocks from temp progress file.")
                except Exception as e:
                    print(f"[Warning] Failed to load resume data from temp file: {e}")
        
        # Open output file, and optionally CoT file
        # 增量模式使用追加写入，否则覆盖写入
        output_mode = 'a' if (args.resume and skip_blocks_from_output > 0) else 'w'
        
        # Prepare Temp Output File (Append or Create)
        temp_file_mode = 'a' if (args.resume and len(precalculated_temp) > 0 and resume_config_matched) else 'w'
        temp_progress_file = open(temp_progress_path, temp_file_mode, encoding='utf-8', buffering=1)
        
        # If starting fresh, write fingerprint
        if temp_file_mode == 'w':
            temp_progress_file.write(json.dumps({"type": "fingerprint", "hash": config_hash}) + "\n")
            temp_progress_file.flush()
        
        cot_context = open(cot_path, 'w', encoding='utf-8', buffering=1) if args.save_cot else nullcontext()
        with open(output_path, output_mode, encoding='utf-8', buffering=1) as f_out, \
             cot_context as f_cot:

            # If resuming from output file, write existing content first
            if skip_blocks_from_output > 0 and output_mode == 'a':
                for line in existing_content:
                    f_out.write(line + '\n')
                # Update initial progress based on existing output
                total_lines += len(existing_content)
                total_out_chars += sum(len(l) for l in existing_content)
                # We don't have source chars for these, so we'll just use the output lines/chars for progress
                # The `current` and `percent` will be based on blocks, so this is fine.
            
            # ========================================
            # Parallel Worker Function
            # ========================================
            def process_block_task(block_idx: int, block: object, strict_mode: bool = False):
                """Worker Task"""
                logger.info(f"[Block {block_idx+1}/{len(blocks)}] Starting translation...")
                start_block_time = time.time()
                if not block.prompt_text or not block.prompt_text.strip():
                     return {
                        "success": True,
                        "block_idx": block_idx,
                        "src_text": "",
                        "out_text": "",
                        "preview_text": "",
                        "cot": "",
                        "raw_output": "",
                        "warnings": [],
                        "lines_count": 0,
                        "chars_count": 0, # Source chars
                        "cot_chars": 0,
                        "usage": None
                    }

                try:
                    # Pause Check (Worker level sleeping)
                    pause_file = output_path + ".pause"
                    while os.path.exists(pause_file):
                         time.sleep(1)
                    
                    # Pre-processing using Unified RuleProcessor
                    logger.debug(f"[Block {block_idx+1}] Pre-processing start (len: {len(block.prompt_text)})")
                    processed_src_text = pre_processor.process(block.prompt_text, strict_line_count=strict_mode)
                    logger.debug(f"[Block {block_idx+1}] After Pre-rules: {len(processed_src_text)} chars")
                    
                    processed_src_text = Normalizer.normalize(processed_src_text)
                    
                    # Thread-Safe Text Protector (Local instantiation)
                    # Use custom_protector_patterns captured from outer scope
                    local_protector = None
                    if args.text_protect:
                         local_protector = TextProtector(patterns=custom_protector_patterns)
                         logger.debug(f"[Block {block_idx+1}] [Experimental] Protection start")
                         processed_src_text = local_protector.protect(processed_src_text)
                         logger.debug(f"[Block {block_idx+1}] [Experimental] After Protection: {len(processed_src_text)} chars")
                    
                    # Build Prompt
                    messages = prompt_builder.build_messages(
                        processed_src_text, 
                        enable_cot=args.debug,
                        preset=args.preset
                    )
                    
                    # Retry Strategy Variables
                    max_retries = args.max_retries
                    best_result = None # (parsed_lines, cot_content, raw_output, coverage)
                    retry_reason = None
                    last_missed_terms = []
                    last_coverage = 0.0
                    
                    final_parsed_lines = None
                    final_cot = ""
                    final_raw = ""

                    for attempt in range(max_retries + 1):
                        # Dynamic Parameters Calculation
                        if retry_reason == 'glossary':
                            current_temp = max(args.temperature - (attempt * args.retry_temp_boost), 0.3)
                            current_rep_base = args.rep_penalty_base + (attempt * args.retry_rep_boost)
                        else:
                            current_temp = min(args.temperature + (attempt * args.retry_temp_boost), 1.2)
                            current_rep_base = args.rep_penalty_base + (attempt * args.retry_rep_boost)

                        # Feedback Injection
                        messages_for_attempt = messages
                        if attempt > 0 and args.retry_prompt_feedback and glossary and last_missed_terms and retry_reason == 'glossary':
                            feedback = build_retry_feedback(last_missed_terms, last_coverage)
                            if feedback:
                                messages_for_attempt = messages.copy()
                                for j in range(len(messages_for_attempt) - 1, -1, -1):
                                    if messages_for_attempt[j].get("role") == "user":
                                        messages_for_attempt[j] = {
                                            "role": "user",
                                            "content": messages_for_attempt[j]["content"] + feedback
                                        }
                                        break
                        
                        accumulated_output = ""
                        # Stream Callback
                        def on_stream_chunk(chunk):
                            nonlocal accumulated_output
                            accumulated_output += chunk
                            if "<think>" in chunk or "</think>" in chunk or (accumulated_output.strip().startswith("<think>") and not "</think>" in accumulated_output):
                                try:
                                    with stdout_lock:
                                        sys.stdout.write(f"\nJSON_THINK_DELTA:{json.dumps(chunk, ensure_ascii=False)}\n")
                                        sys.stdout.flush()
                                except: pass

                        # Inference
                        # engine.chat_completion returns (text, usage)
                        full_response_text, block_usage = engine.chat_completion(
                            messages=messages_for_attempt,
                            temperature=current_temp,
                            stream=True,
                            stream_callback=on_stream_chunk,
                            rep_base=current_rep_base,
                            rep_max=args.rep_penalty_max,
                            block_id=block_idx + 1
                        )
                        
                        raw_output = full_response_text
                        parsed_lines, cot_content = response_parser.parse(raw_output or "", expected_count=0)
                        
                        # Fix lines count to be consistent (non-empty lines)
                        has_content = parsed_lines and any(line.strip() for line in parsed_lines)

                        # 1. Empty Output Guard
                        if not has_content:
                            retry_reason = 'empty'
                            if attempt < max_retries:
                                retry_data = {
                                    'block': block_idx + 1, 
                                    'attempt': attempt + 1, 
                                    'type': 'empty', 
                                    'temp': round(current_temp, 2)
                                }
                                safe_print_json("JSON_RETRY", retry_data)
                                continue
                            else:
                                break # Failed all retries
                        
                        # 2. Quality Checks
                        should_retry = False
                        
                        # Glossary Coverage Check
                        if glossary and args.output_hit_threshold > 0:
                            translated_text = '\n'.join(parsed_lines)
                            passed, coverage, cot_coverage, hit, total = calculate_glossary_coverage(
                                block.prompt_text, translated_text, glossary, cot_content,
                                args.output_hit_threshold, args.cot_coverage_threshold
                            )
                            last_coverage = coverage
                            last_missed_terms = get_missed_terms(block.prompt_text, translated_text, glossary)
                            
                            if best_result is None or coverage > best_result[3]:
                                best_result = (parsed_lines.copy(), cot_content, raw_output, coverage)

                            if total > 0 and not passed:
                                retry_reason = 'glossary'
                                coverage_attempts = min(attempt + 1, args.coverage_retries)
                                if coverage_attempts < args.coverage_retries and attempt < max_retries:
                                    retry_data = {
                                        'block': block_idx + 1, 
                                        'attempt': attempt + 1, 
                                        'type': 'glossary',
                                        'coverage': round(coverage, 1),
                                        'temp': round(current_temp, 2),
                                        'missed_count': len(last_missed_terms)
                                    }
                                    safe_print_json("JSON_RETRY", retry_data)
                                    should_retry = True
                                elif best_result and best_result[3] > coverage:
                                    # Use best result if current one is worse and we are out of glossary retries
                                    parsed_lines, cot_content, raw_output, _ = best_result
                        
                        # Line Count Check
                        if not should_retry and args.line_check:
                            src_line_count = len([l for l in block.prompt_text.splitlines() if l.strip()])
                            dst_line_count = len([l for l in parsed_lines if l.strip()])
                            diff = abs(dst_line_count - src_line_count)
                            pct_diff = diff / max(1, src_line_count)
                            
                            if diff > args.line_tolerance_abs or pct_diff > args.line_tolerance_pct:
                                if attempt < max_retries:
                                    retry_reason = 'line_check'
                                    retry_data = {
                                        'block': block_idx + 1, 
                                        'attempt': attempt + 1, 
                                        'type': 'line_check'
                                    }
                                    safe_print_json("JSON_RETRY", retry_data)
                                    should_retry = True
                        
                        if should_retry:
                            continue
                        
                        # Success or acceptable quality
                        final_parsed_lines = parsed_lines
                        block_elapsed = time.time() - start_block_time
                        speed_cps = len(accumulated_output) / max(0.1, block_elapsed)
                        logger.info(f"[Block {block_idx+1}/{len(blocks)}] Finished in {block_elapsed:.1f}s (Chars: {len(accumulated_output)}, Speed: {speed_cps:.1f} chars/s)")
                        final_cot = cot_content
                        final_raw = raw_output
                        break

                    # Final Fallback if all retries failed
                    if final_parsed_lines is None:
                        if best_result:
                            final_parsed_lines, final_cot, final_raw, _ = best_result
                        else:
                            final_parsed_lines = ["[翻译失败]"] + block.prompt_text.split('\n')
                            final_cot = ""
                            final_raw = ""

                    # Post-Process (Consolidated Pipeline)
                    # 1. Join decoded lines
                    base_text = '\n'.join(final_parsed_lines)
                    
                    # 2. Unified Rule Processor Application (Integrated Restoration)
                    # Restoration happens inside process() if 'restore_protection' rule exists
                    logger.debug(f"[Block {block_idx+1}] Post-processing start (len: {len(base_text)})")
                    processed_text = post_processor.process(base_text, src_text=block.prompt_text, protector=local_protector, strict_line_count=strict_mode)
                    logger.debug(f"[Block {block_idx+1}] Post-processing finished (len: {len(processed_text)})")
                    
                    # Ensure Consistency: Preview = Output = Cache
                    preview_text = processed_text
                        
                    # Quality Check Warnings (for cache)
                    warnings = []
                    try:
                        qc = QualityChecker(glossary=glossary)
                        warnings = qc.check_output(
                            [l for l in block.prompt_text.split('\n') if l.strip()], 
                            [l for l in preview_text.split('\n') if l.strip()], 
                            source_lang="ja"
                        )
                    except Exception as e:
                        print(f"[Warning] Quality Check failed: {e}")
                    

                    
                    # Emit Preview JSON for GUI -> MOVED TO MAIN LOOP (Ordered)
                    # preview_data = { ... }
                    # safe_print_json("JSON_PREVIEW_BLOCK", preview_data)

                    return {
                        "success": True,
                        "block_idx": block_idx,
                        "src_text": block.prompt_text,
                        "out_text": processed_text,
                        "preview_text": preview_text,
                        "cot": final_cot,
                        "raw_output": final_raw,
                        "warnings": warnings,
                        "lines_count": len([l for l in processed_text.splitlines() if l.strip()]),
                        "chars_count": len(block.prompt_text),
                        "cot_chars": len(final_cot),
                        "usage": block_usage
                    }



                except Exception as e:
                    return {
                        "success": False,
                        "error": str(e),
                        "block_idx": block_idx,
                        "src_text": block.prompt_text
                    }

            def restore_block_task(block_idx: int, stored_result: Dict):
                """Dummy task to restore pre-calculated result immediately."""
                return stored_result

            
            # Stats Initialization
            total_out_chars = 0      # All Output Chars (Main + Summary etc)
            total_cot_chars = 0      # CoT Thinking Chars
            total_source_chars = 0   # Source Chars
            total_source_lines = 0   # Source Lines
            total_lines = 0          # Output Lines
            total_prompt_tokens = 0  # Input/Prompt Tokens (from Engine)
            total_gen_tokens = 0     # Generation Tokens (from Engine)
            
            # ...
            # Main Execution Loop
            # ========================================
            
            # Use ThreadPoolExecutor
            max_workers = args.concurrency
            executor = ThreadPoolExecutor(max_workers=max_workers)
            
            # Submit all tasks
            # We maintain a map of future -> index
            future_to_index = {}
            
            # --- Ordered Buffer for Writing & Reconstruction ---
            all_results = [None] * len(blocks) # Pre-fill for structural reconstruction
            
            # Determine if we should use strict line count (structured docs)
            _, file_ext = os.path.splitext(input_path)
            is_structured_doc = file_ext.lower() in ['.epub', '.srt']

            # [Audit Fix] Fill skipped blocks from output file to support EPUB/SRT reconstruction
            # Only for structured docs as TXT streaming is sufficient and reconstruction causes drift
            if skip_blocks_from_output > 0 and existing_content and is_structured_doc:
                print(f"[Resume] Rebuilding memory state for {skip_blocks_from_output} blocks (Structured Doc)...")
                current_line_ptr = 0
                for idx in range(skip_blocks_from_output):
                    # Conservative reconstruction based on source block line counts
                    # This assumes strict_line_count was used in previous run
                    block_lines_count = blocks[idx].prompt_text.count('\n') + 1
                    block_lines = existing_content[current_line_ptr : current_line_ptr + block_lines_count]
                    
                    if block_lines:
                        all_results[idx] = {
                            "success": True,
                            "out_text": '\n'.join(block_lines),
                            "preview_text": '\n'.join(block_lines),
                            "block_idx": idx,
                            "is_restorer": True,
                            "warnings": [],
                            "src_text": blocks[idx].prompt_text,
                            "cot_chars": 0,
                            "usage": {}
                        }
                    else:
                        print(f"[Resume] Warning: Could not find content for block {idx} in existing output.")
                    
                    current_line_ptr += block_lines_count
            elif skip_blocks_from_output > 0:
                print(f"[Resume] Skipping memory reconstruction for unstructured document (TXT).")

            if is_structured_doc:
                print(f"[Init] Detected structured document ({file_ext}). Enabling strict line count mode.")

            print(f"Starting execution with {max_workers} threads...")
            
            for i, block in enumerate(blocks):
                # Resume skip
                if i < skip_blocks_from_output:
                    continue
                
                # Check temp progress
                if i in precalculated_temp:
                    future = executor.submit(restore_block_task, i, precalculated_temp[i])
                    print(f"  - Restoring Block {i+1} from temp file...")
                else:
                    # Pass is_structured_doc to task if needed, but RuleProcessor is globally initialized
                    # Actually we should initialize the processors WITH this flag or pass it to process()
                    future = executor.submit(process_block_task, i, block, is_structured_doc)
                
                future_to_index[future] = i
            
            # Fill skipped blocks if they exist in precalculated_temp or if we need to regenerate?
            # For now, if we resume, we rely on the fact that doc.save usually only needs the new blocks?
            # NO, for EPUB/SRT, we need THE ENTIRE document.
            if skip_blocks_from_output > 0:
                print(f"[Resume] Warning: Full document reconstruction (EPUB/SRT) requires all blocks. Currently only new blocks are in memory.")
            
            results_buffer = {}
            next_write_idx = skip_blocks_from_output
            
            # 统计修正：过滤掉空块（用于负载均衡的占位块）
            effective_blocks_indices = [idx for idx, b in enumerate(blocks) if b.prompt_text.strip()]
            total_tasks_count = len(future_to_index)
            effective_total = len(effective_blocks_indices)
            completed_count = 0 
            effective_completed = 0
            
            for future in as_completed(future_to_index):
                block_idx = future_to_index[future]
                try:
                    result = future.result() 
                except Exception as e:
                    safe_print(f"Worker Error for Block {block_idx+1}: {e}")
                    result = {
                        "success": False,
                        "error": str(e),
                        "block_idx": block_idx,
                        "src_text": blocks[block_idx].prompt_text if block_idx < len(blocks) else "Unknown",
                        "out_text": "[Worker Exception]",
                        "preview_text": "[Worker Exception]",
                        "cot": "",
                        "raw_output": "",
                        "warnings": [],
                        "lines_count": 0,
                        "chars_count": 0,
                        "cot_chars": 0,
                        "usage": None
                    }
                
                # 放入缓冲区
                results_buffer[block_idx] = result
                completed_count += 1
                if result["src_text"].strip():
                    effective_completed += 1
                    
                # 1. 更新统计数据 (只要任务完成就更新，不论顺序)
                if result["success"]:
                    total_out_chars += len(result["out_text"])
                    total_lines += result["lines_count"]
                    total_cot_chars += result["cot_chars"]
                    
                    # Accumulate Tokens
                    if result.get("usage"):
                        total_prompt_tokens += result["usage"].get("prompt_tokens", 0)
                        total_gen_tokens += result["usage"].get("completion_tokens", 0)
                    
                    if block_idx not in precalculated_temp:
                        try:
                            temp_line = json.dumps(result, ensure_ascii=False)
                            temp_progress_file.write(temp_line + "\n")
                            temp_progress_file.flush()
                        except: pass
                    
                    block_src_text = result["src_text"]
                    total_source_chars += len(block_src_text)
                    total_source_lines += len([l for l in block_src_text.splitlines() if l.strip()])
                    
                    # 发送进度 JSON
                    elapsed_so_far = max(0.1, time.time() - start_time)
                    current_speed_chars = (total_out_chars + total_cot_chars) / elapsed_so_far
                    
                    # 计算剩余时间
                    avg_time_per_block = elapsed_so_far / completed_count
                    remaining_time = (total_tasks_count - completed_count) * avg_time_per_block
                    
                    progress_data = {
                         "current": effective_completed + len([idx for idx in range(skip_blocks_from_output) if blocks[idx].prompt_text.strip()]),
                         "ordered_current": next_write_idx,
                         "total": effective_total,
                         "percent": ((effective_completed + len([idx for idx in range(skip_blocks_from_output) if blocks[idx].prompt_text.strip()])) / max(1, effective_total)) * 100,
                         "total_chars": total_out_chars,
                         "total_lines": total_lines,
                         "source_chars": total_source_chars,
                         "source_lines": total_source_lines, 
                         "speed_chars": round(current_speed_chars, 1),
                         "speed_lines": round(total_lines / elapsed_so_far, 2),
                         "speed_gen": round(total_gen_tokens / elapsed_so_far, 1),
                         "speed_eval": round(total_prompt_tokens / elapsed_so_far, 1),
                         "total_tokens": total_gen_tokens, # Cumulative gen tokens
                         "elapsed": elapsed_so_far,
                         "remaining": int(remaining_time)
                    }
                    
                    now = time.time()
                    if (now - last_progress_time > 0.1) or (completed_count == total_tasks_count):
                        safe_print_json("JSON_PROGRESS", progress_data)
                        last_progress_time = now

                # 2. 顺序写入文件 (关键修复：检查缓冲区是否有接下来的序号)
                while next_write_idx in results_buffer:
                    res = results_buffer.pop(next_write_idx)
                    curr_disp = next_write_idx + 1
                    
                    if res["success"]:
                        # 发送预览
                        preview_data = {"block": curr_disp, "src": res['src_text'], "output": res['preview_text']}
                        safe_print_json("JSON_PREVIEW_BLOCK", preview_data)
                        
                        # 更新校对缓存
                        if translation_cache:
                            w_types = [w['type'] for w in res["warnings"]] if res["warnings"] else []
                            translation_cache.add_block(next_write_idx, res["src_text"], res["preview_text"], w_types, res["cot"])
                        
                        # 写入文件
                        f_out.write(res["out_text"] + "\n")
                        if args.mode == "doc": f_out.write("\n")
                        
                        if args.save_cot and res["cot"]:
                            f_cot.write(f"[MURASAKI] ========== Block {curr_disp} ==========\n")
                            f_cot.write(res["raw_output"] + "\n\n")
                    else:
                        f_out.write(f"\n[Block {curr_disp} Failed]\n")
                    
                    f_out.flush()
                    
                    # Store for final save if needed (e.g. EPUB)
                    all_results[next_write_idx] = res
                    next_write_idx += 1

            executor.shutdown(wait=False)
            
            # Final Save using Document Handler (for structure reconstruction)
            # Only for structured docs. TXT is already saved via streaming f_out.
            if is_structured_doc:
                try:
                    # We need TextBlock objects with prompt_text = translated_text
                    # to satisfy the doc.save(output_path, blocks) signature
                    from murasaki_translator.core.chunker import TextBlock
                    translated_blocks = []
                    for i, res in enumerate(all_results):
                        if res and res.get('out_text') is not None:
                            translated_blocks.append(TextBlock(
                                id=i,
                                prompt_text=res['out_text'],
                                metadata=blocks[i].metadata
                            ))
                        else:
                            # Placeholder for failed or missing blocks
                            translated_blocks.append(blocks[i])
                    
                    doc.save(output_path, translated_blocks)
                    print(f"[Success] Structured document rebuilt: {output_path}")
                except Exception as e:
                    print(f"[Warning] Final document reconstruction failed: {e}")
            else:
                print(f"[Success] Translation completed. Output saved to: {output_path}")

        # 任务结束后的总结
        total_time = time.time() - start_time
        if translation_cache:
            m_name = os.path.basename(args.model) if args.model else "Unknown"
            translation_cache.save(model_name=m_name, glossary_path=args.glossary or "", concurrency=args.concurrency)

        # 发送最终 JSON 统计
        final_stats = {
            "sourceLines": total_source_lines, "sourceChars": total_source_chars,
            "outputLines": total_lines, "outputChars": total_out_chars,
            "totalTime": round(total_time, 2), "avgSpeed": round(total_out_chars / total_time, 1) if total_time > 0 else 0
        }
        safe_print_json("JSON_FINAL", final_stats)
        
        # 清理临时文件
        if (completed_count + skip_blocks_from_output) >= len(blocks):
            try:
                temp_progress_file.close()
                if os.path.exists(temp_progress_path): os.remove(temp_progress_path)
            except: pass

    except KeyboardInterrupt:
        print("\n[System] Interrupted by user. Shutting down immediately...")
        if 'executor' in locals():
            executor.shutdown(wait=False, cancel_futures=True)
        if engine: 
            engine.stop_server()
    except Exception as e:
        print(f"\n[System] Critical Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if 'temp_progress_file' in locals():
            try:
                temp_progress_file.close()
            except: pass
        if engine:
            engine.stop_server()

if __name__ == "__main__":
    main()
