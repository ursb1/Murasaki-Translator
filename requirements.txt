# Core ML Framework
torch>=2.1.0
torchvision>=0.16.0
torchaudio>=2.1.0

# Transformers & LLM
transformers>=4.45.0
accelerate>=0.34.0
datasets>=2.21.0
peft>=0.13.0
trl>=0.11.0

# LLaMA-Factory (Core Training Framework)
llamafactory>=0.9.0

# Memory Optimization
deepspeed>=0.14.0
bitsandbytes>=0.43.0

# Flash Attention 2 (CRITICAL for efficiency)
# Note: Requires CUDA 12.1+ and manual compilation on Windows
# Install with: pip install flash-attn --no-build-isolation
flash-attn>=2.5.0

# Tokenizers & Utilities
sentencepiece>=0.2.0
tiktoken>=0.7.0
protobuf>=4.25.0

# Evaluation & Metrics
rouge-score>=0.1.2
nltk>=3.8.0
sacrebleu>=2.4.0

# Utilities
tqdm>=4.66.0
wandb>=0.18.0
tensorboard>=2.17.0
einops>=0.8.0
packaging>=24.0

# Data Processing
pandas>=2.2.0
numpy>=1.26.0
jieba>=0.42.1  # Chinese tokenization for ACGN content

# Japanese NLP (for Glossary Injection Pipeline)
sudachipy>=0.6.8
sudachidict-core>=20240109
fugashi>=1.3.0
unidic-lite>=1.0.8

# Embedding Models
sentence-transformers>=2.2.0  # For BGE-M3 alignment

# Formatted Document Reconstruction (EPUB/HTML)
lxml>=4.9.0
beautifulsoup4>=4.12.0
ebooklib>=0.18
