name: Build & Release

on:
  push:
    tags:
      - 'v*'
  workflow_dispatch:
    inputs:
      version:
        description: 'Version to build (e.g., v1.4.1)'
        required: false

permissions:
  contents: write

env:
  LLAMA_CPP_VERSION: "b7770"
  PYTHON_VERSION: "3.11.9"
  NODE_VERSION: "20"

jobs:
  # ============================================
  # Windows CUDA Build
  # ============================================
  build-win-cuda:
    runs-on: windows-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: GUI/package-lock.json

      - name: Download llama.cpp CUDA binaries
        shell: pwsh
        run: |
          $url = "https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_CPP_VERSION }}/llama-${{ env.LLAMA_CPP_VERSION }}-bin-win-cuda-12.4-x64.zip"
          Write-Host "Downloading: $url"
          Invoke-WebRequest -Uri $url -OutFile llama-cuda.zip
          New-Item -ItemType Directory -Force -Path middleware/bin/win-cuda
          Expand-Archive -Path llama-cuda.zip -DestinationPath middleware/bin/win-cuda -Force
          # å¤„ç†å¯èƒ½çš„åµŒå¥—ç›®å½•
          $nested = Get-ChildItem middleware/bin/win-cuda -Directory | Where-Object { $_.Name -like "llama-*" }
          if ($nested) {
            Get-ChildItem $nested.FullName | Move-Item -Destination middleware/bin/win-cuda -Force
            Remove-Item $nested.FullName -Recurse
          }
          
          # ä¸‹è½½ NVIDIA cuBLAS è¿è¡Œæ—¶ DLLï¼ˆcublasLt64_12.dll, cublas64_12.dllï¼‰
          # llama.cpp CUDA ç‰ˆæœ¬éœ€è¦æ­¤ DLLï¼Œä½†å®˜æ–¹åŒ…ä¸å†åŒ…å«
          $cublasUrl = "https://developer.download.nvidia.com/compute/cuda/redist/libcublas/windows-x86_64/libcublas-windows-x86_64-12.4.2.65-archive.zip"
          Write-Host "Downloading cuBLAS: $cublasUrl"
          Invoke-WebRequest -Uri $cublasUrl -OutFile cublas.zip
          Expand-Archive -Path cublas.zip -DestinationPath cublas-temp -Force
          Copy-Item "cublas-temp/libcublas-windows-x86_64-12.4.2.65-archive/bin/cublasLt64_12.dll" -Destination middleware/bin/win-cuda/ -Force
          Copy-Item "cublas-temp/libcublas-windows-x86_64-12.4.2.65-archive/bin/cublas64_12.dll" -Destination middleware/bin/win-cuda/ -Force
          Remove-Item cublas-temp -Recurse -Force
          
          # ä¸‹è½½ NVIDIA CUDA è¿è¡Œæ—¶ DLLï¼ˆcudart64_12.dllï¼‰
          $cudartUrl = "https://developer.download.nvidia.com/compute/cuda/redist/cuda_cudart/windows-x86_64/cuda_cudart-windows-x86_64-12.4.99-archive.zip"
          Write-Host "Downloading CUDA Runtime: $cudartUrl"
          Invoke-WebRequest -Uri $cudartUrl -OutFile cudart.zip
          Expand-Archive -Path cudart.zip -DestinationPath cudart-temp -Force
          Copy-Item "cudart-temp/cuda_cudart-windows-x86_64-12.4.99-archive/bin/cudart64_12.dll" -Destination middleware/bin/win-cuda/ -Force
          Remove-Item cudart-temp -Recurse -Force
          
          Get-ChildItem middleware/bin/win-cuda

      - name: Setup Embedded Python
        shell: pwsh
        run: |
          $url = "https://www.python.org/ftp/python/${{ env.PYTHON_VERSION }}/python-${{ env.PYTHON_VERSION }}-embed-amd64.zip"
          Write-Host "Downloading Python: $url"
          Invoke-WebRequest -Uri $url -OutFile python-embed.zip
          Expand-Archive -Path python-embed.zip -DestinationPath python_env -Force
          
          # Enable site-packages
          $pthFile = Get-ChildItem -Path python_env -Filter "python*._pth"
          (Get-Content $pthFile.FullName) -replace '#import site', 'import site' | Set-Content $pthFile.FullName
          
          # Install pip
          Invoke-WebRequest -Uri "https://bootstrap.pypa.io/get-pip.py" -OutFile get-pip.py
          python_env/python.exe get-pip.py --no-warn-script-location
          
          # Install dependencies (åŒ…å« server æ¨¡å—)
          python_env/python.exe -m pip install -r middleware/requirements.txt --target python_env/Lib/site-packages --no-warn-script-location
          python_env/python.exe -m pip install -r middleware/server/requirements.txt --target python_env/Lib/site-packages --no-warn-script-location
          python_env/python.exe -c "import fastapi, uvicorn, httpx, multipart, pydantic; print('embedded python deps ok')"

      - name: Install Node dependencies
        working-directory: GUI
        run: npm install

      - name: Prepare packaged docs
        shell: pwsh
        run: |
          New-Item -ItemType Directory -Force -Path release | Out-Null
          Copy-Item ".github/release-docs/README-windows-cuda.md" -Destination "release/readme.md" -Force
          Copy-Item "LICENSE" -Destination "release/LICENSE.Murasaki Translator.txt" -Force

      - name: Build
        working-directory: GUI
        run: npm run build:win
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Rename artifact
        shell: pwsh
        run: |
          # Rename output zip
          Get-ChildItem GUI/dist/*.zip | ForEach-Object {
            $newName = "Murasaki-Translator-${{ github.ref_name }}-win-cuda-x64.zip"
            Rename-Item $_.FullName -NewName $newName
          }
      - name: Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: Murasaki-win-cuda-x64
          path: GUI/dist/*-cuda-x64.zip
          retention-days: 7

  # ============================================
  # Windows Vulkan Build
  # ============================================
  build-win-vulkan:
    runs-on: windows-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: GUI/package-lock.json

      - name: Download llama.cpp Vulkan binaries
        shell: pwsh
        run: |
          $url = "https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_CPP_VERSION }}/llama-${{ env.LLAMA_CPP_VERSION }}-bin-win-vulkan-x64.zip"
          Write-Host "Downloading: $url"
          Invoke-WebRequest -Uri $url -OutFile llama-vulkan.zip
          New-Item -ItemType Directory -Force -Path middleware/bin/win-vulkan
          Expand-Archive -Path llama-vulkan.zip -DestinationPath middleware/bin/win-vulkan -Force
          # å¤„ç†å¯èƒ½çš„åµŒå¥—ç›®å½•
          $nested = Get-ChildItem middleware/bin/win-vulkan -Directory | Where-Object { $_.Name -like "llama-*" }
          if ($nested) {
            Get-ChildItem $nested.FullName | Move-Item -Destination middleware/bin/win-vulkan -Force
            Remove-Item $nested.FullName -Recurse
          }

      - name: Setup Embedded Python
        shell: pwsh
        run: |
          $url = "https://www.python.org/ftp/python/${{ env.PYTHON_VERSION }}/python-${{ env.PYTHON_VERSION }}-embed-amd64.zip"
          Invoke-WebRequest -Uri $url -OutFile python-embed.zip
          Expand-Archive -Path python-embed.zip -DestinationPath python_env -Force
          $pthFile = Get-ChildItem -Path python_env -Filter "python*._pth"
          (Get-Content $pthFile.FullName) -replace '#import site', 'import site' | Set-Content $pthFile.FullName
          Invoke-WebRequest -Uri "https://bootstrap.pypa.io/get-pip.py" -OutFile get-pip.py
          python_env/python.exe get-pip.py --no-warn-script-location
          python_env/python.exe -m pip install -r middleware/requirements.txt --target python_env/Lib/site-packages --no-warn-script-location
          python_env/python.exe -m pip install -r middleware/server/requirements.txt --target python_env/Lib/site-packages --no-warn-script-location
          python_env/python.exe -c "import fastapi, uvicorn, httpx, multipart, pydantic; print('embedded python deps ok')"

      - name: Install Node dependencies
        working-directory: GUI
        run: npm install

      - name: Prepare packaged docs
        shell: pwsh
        run: |
          New-Item -ItemType Directory -Force -Path release | Out-Null
          Copy-Item ".github/release-docs/README-windows-vulkan.md" -Destination "release/readme.md" -Force
          Copy-Item "LICENSE" -Destination "release/LICENSE.Murasaki Translator.txt" -Force

      - name: Build
        working-directory: GUI
        run: npm run build:win
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Rename artifact
        shell: pwsh
        run: |
          # Rename output zip
          Get-ChildItem GUI/dist/*.zip | ForEach-Object {
            $newName = "Murasaki-Translator-${{ github.ref_name }}-win-vulkan-x64.zip"
            Rename-Item $_.FullName -NewName $newName
          }
      - name: Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: Murasaki-win-vulkan-x64
          path: GUI/dist/*-vulkan-x64.zip
          retention-days: 7

  # ============================================
  # macOS Build (Universal)
  # ============================================
  build-mac:
    runs-on: macos-14  # Apple Silicon runner
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: GUI/package-lock.json

      - name: Download llama.cpp macOS binaries
        run: |
          # Metal (ARM64)
          curl -L -o llama-metal.tar.gz "https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_CPP_VERSION }}/llama-${{ env.LLAMA_CPP_VERSION }}-bin-macos-arm64.tar.gz"
          mkdir -p middleware/bin/darwin-metal
          tar -xzf llama-metal.tar.gz -C middleware/bin/darwin-metal --strip-components=1 || tar -xzf llama-metal.tar.gz -C middleware/bin/darwin-metal
          
          # å¤„ç†å¤šå±‚åµŒå¥—ç›®å½•
          if [ -d "middleware/bin/darwin-metal/build/bin" ]; then
            mv middleware/bin/darwin-metal/build/bin/* middleware/bin/darwin-metal/ 2>/dev/null || true
            rm -rf middleware/bin/darwin-metal/build
          elif [ -d "middleware/bin/darwin-metal/bin" ]; then
            mv middleware/bin/darwin-metal/bin/* middleware/bin/darwin-metal/ 2>/dev/null || true
            rm -rf middleware/bin/darwin-metal/bin
          fi
          
          # é€’å½’æŸ¥æ‰¾å¹¶ç§»åŠ¨ llama-server åˆ°é¡¶å±‚
          SERVER_PATH=$(find middleware/bin/darwin-metal -name "llama-server" -type f | head -1)
          if [ -n "$SERVER_PATH" ] && [ "$SERVER_PATH" != "middleware/bin/darwin-metal/llama-server" ]; then
            echo "Moving llama-server from $SERVER_PATH to top level"
            mv "$SERVER_PATH" middleware/bin/darwin-metal/
          fi
          chmod +x middleware/bin/darwin-metal/llama-server 2>/dev/null || true
          
          # x64 (Intel)
          curl -L -o llama-x64.tar.gz "https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_CPP_VERSION }}/llama-${{ env.LLAMA_CPP_VERSION }}-bin-macos-x64.tar.gz"
          mkdir -p middleware/bin/darwin-x64
          tar -xzf llama-x64.tar.gz -C middleware/bin/darwin-x64 --strip-components=1 || tar -xzf llama-x64.tar.gz -C middleware/bin/darwin-x64
          
          # å¤„ç†å¤šå±‚åµŒå¥—ç›®å½•
          if [ -d "middleware/bin/darwin-x64/build/bin" ]; then
            mv middleware/bin/darwin-x64/build/bin/* middleware/bin/darwin-x64/ 2>/dev/null || true
            rm -rf middleware/bin/darwin-x64/build
          elif [ -d "middleware/bin/darwin-x64/bin" ]; then
            mv middleware/bin/darwin-x64/bin/* middleware/bin/darwin-x64/ 2>/dev/null || true
            rm -rf middleware/bin/darwin-x64/bin
          fi
          
          # é€’å½’æŸ¥æ‰¾å¹¶ç§»åŠ¨ llama-server åˆ°é¡¶å±‚
          SERVER_PATH=$(find middleware/bin/darwin-x64 -name "llama-server" -type f | head -1)
          if [ -n "$SERVER_PATH" ] && [ "$SERVER_PATH" != "middleware/bin/darwin-x64/llama-server" ]; then
            echo "Moving llama-server from $SERVER_PATH to top level"
            mv "$SERVER_PATH" middleware/bin/darwin-x64/
          fi
          chmod +x middleware/bin/darwin-x64/llama-server 2>/dev/null || true
          
          # éªŒè¯äºŒè¿›åˆ¶å­˜åœ¨
          echo "=== Metal (ARM64) binaries ==="
          ls -la middleware/bin/darwin-metal/
          test -f middleware/bin/darwin-metal/llama-server && echo "âœ… Metal llama-server found" || echo "âŒ Metal llama-server NOT found"
          echo "=== x64 (Intel) binaries ==="
          ls -la middleware/bin/darwin-x64/
          test -f middleware/bin/darwin-x64/llama-server && echo "âœ… x64 llama-server found" || echo "âŒ x64 llama-server NOT found"

      - name: Setup uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true

      - name: Install Python dependencies
        run: |
          python3 -m venv .venv
          source .venv/bin/activate
          pip install -r middleware/requirements.txt
          pip install -r middleware/server/requirements.txt
          python -c "import fastapi, uvicorn, httpx, multipart, pydantic; print('python deps ok')"

      - name: Build Python Engine (PyInstaller)
        run: |
          source .venv/bin/activate
          pip install pyinstaller
          cd middleware
          
          # åˆ›å»º PyInstaller spec ç”¨äºŽæ‰“åŒ…
          pyinstaller --onefile --name murasaki-engine \
            --hidden-import opencc \
            --hidden-import chardet \
            --hidden-import regex \
            --hidden-import pynvml \
            --hidden-import lxml \
            --hidden-import lxml.etree \
            --hidden-import ebooklib \
            --hidden-import ebooklib.epub \
            --collect-all opencc \
            --add-data "murasaki_translator:murasaki_translator" \
            --add-data "rule_processor.py:." \
            murasaki_translator/main.py
          
          # ç§»åŠ¨åˆ° bin ç›®å½•ä¾› Electron ä½¿ç”¨
          mkdir -p bin/python-bundle
          mv dist/murasaki-engine bin/python-bundle/
          chmod +x bin/python-bundle/murasaki-engine
          
          echo "Built Python bundle:"
          ls -la bin/python-bundle/

      - name: Install Node dependencies
        working-directory: GUI
        run: npm install

      - name: Prepare packaged docs
        run: |
          mkdir -p release
          cp .github/release-docs/README-macos.md release/readme.md
          cp LICENSE "release/LICENSE.Murasaki Translator.txt"

      - name: Build for ARM64
        working-directory: GUI
        run: |
          npm run build
          npx electron-builder --mac --arm64
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Build for x64
        working-directory: GUI
        run: npx electron-builder --mac --x64
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: Murasaki-mac
          path: GUI/dist/*.dmg
          retention-days: 7

  # ============================================
  # Linux Build (GUI + CLI)
  # ============================================
  build-linux:
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: GUI/package-lock.json

      - name: Download llama.cpp Linux binaries
        run: |
          # Vulkan ç‰ˆæœ¬ (AMD/Intel GPU)
          curl -L -o llama-vulkan.tar.gz "https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_CPP_VERSION }}/llama-${{ env.LLAMA_CPP_VERSION }}-bin-ubuntu-vulkan-x64.tar.gz"
          mkdir -p middleware/bin/linux-vulkan
          tar -xzf llama-vulkan.tar.gz -C middleware/bin/linux-vulkan --strip-components=1 || tar -xzf llama-vulkan.tar.gz -C middleware/bin/linux-vulkan
          
          # å¤„ç†å¤šå±‚åµŒå¥—ç›®å½• (build/bin/ æˆ–ç›´æŽ¥ bin/)
          if [ -d "middleware/bin/linux-vulkan/build/bin" ]; then
            mv middleware/bin/linux-vulkan/build/bin/* middleware/bin/linux-vulkan/ 2>/dev/null || true
            rm -rf middleware/bin/linux-vulkan/build
          elif [ -d "middleware/bin/linux-vulkan/bin" ]; then
            mv middleware/bin/linux-vulkan/bin/* middleware/bin/linux-vulkan/ 2>/dev/null || true
            rm -rf middleware/bin/linux-vulkan/bin
          fi
          
          # é€’å½’æŸ¥æ‰¾å¹¶ç§»åŠ¨ llama-server åˆ°é¡¶å±‚
          SERVER_PATH=$(find middleware/bin/linux-vulkan -name "llama-server" -type f | head -1)
          if [ -n "$SERVER_PATH" ] && [ "$SERVER_PATH" != "middleware/bin/linux-vulkan/llama-server" ]; then
            echo "Moving llama-server from $SERVER_PATH to top level"
            mv "$SERVER_PATH" middleware/bin/linux-vulkan/
          fi
          chmod +x middleware/bin/linux-vulkan/llama-server 2>/dev/null || true
          
          # CPU ç‰ˆæœ¬ (Fallback / NVIDIA CUDA ç”¨æˆ·éœ€è‡ªè¡Œç¼–è¯‘)
          # æ³¨æ„ï¼šllama.cpp å®˜æ–¹ä¸æä¾› Linux CUDA é¢„ç¼–è¯‘åŒ…
          curl -L -o llama-cpu.tar.gz "https://github.com/ggml-org/llama.cpp/releases/download/${{ env.LLAMA_CPP_VERSION }}/llama-${{ env.LLAMA_CPP_VERSION }}-bin-ubuntu-x64.tar.gz"
          mkdir -p middleware/bin/linux-cpu
          tar -xzf llama-cpu.tar.gz -C middleware/bin/linux-cpu --strip-components=1 || tar -xzf llama-cpu.tar.gz -C middleware/bin/linux-cpu
          
          # å¤„ç†å¤šå±‚åµŒå¥—ç›®å½•
          if [ -d "middleware/bin/linux-cpu/build/bin" ]; then
            mv middleware/bin/linux-cpu/build/bin/* middleware/bin/linux-cpu/ 2>/dev/null || true
            rm -rf middleware/bin/linux-cpu/build
          elif [ -d "middleware/bin/linux-cpu/bin" ]; then
            mv middleware/bin/linux-cpu/bin/* middleware/bin/linux-cpu/ 2>/dev/null || true
            rm -rf middleware/bin/linux-cpu/bin
          fi
          
          # é€’å½’æŸ¥æ‰¾å¹¶ç§»åŠ¨ llama-server åˆ°é¡¶å±‚
          SERVER_PATH=$(find middleware/bin/linux-cpu -name "llama-server" -type f | head -1)
          if [ -n "$SERVER_PATH" ] && [ "$SERVER_PATH" != "middleware/bin/linux-cpu/llama-server" ]; then
            echo "Moving llama-server from $SERVER_PATH to top level"
            mv "$SERVER_PATH" middleware/bin/linux-cpu/
          fi
          chmod +x middleware/bin/linux-cpu/llama-server 2>/dev/null || true
          
          # éªŒè¯äºŒè¿›åˆ¶å­˜åœ¨
          echo "=== Vulkan binaries ==="
          ls -la middleware/bin/linux-vulkan/
          test -f middleware/bin/linux-vulkan/llama-server && echo "âœ… Vulkan llama-server found" || echo "âŒ Vulkan llama-server NOT found"
          echo "=== CPU binaries ==="
          ls -la middleware/bin/linux-cpu/
          test -f middleware/bin/linux-cpu/llama-server && echo "âœ… CPU llama-server found" || echo "âŒ CPU llama-server NOT found"

      - name: Setup uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true

      - name: Install Python dependencies
        run: |
          python3 -m venv .venv
          source .venv/bin/activate
          pip install -r middleware/requirements.txt
          pip install -r middleware/server/requirements.txt
          python -c "import fastapi, uvicorn, httpx, multipart, pydantic; print('python deps ok')"

      - name: Build Python Engine (PyInstaller)
        run: |
          source .venv/bin/activate
          pip install pyinstaller
          cd middleware
          
          # åˆ›å»º PyInstaller spec ç”¨äºŽæ‰“åŒ…ï¼ˆä¸Ž macOS ç›¸åŒï¼‰
          pyinstaller --onefile --name murasaki-engine \
            --hidden-import opencc \
            --hidden-import chardet \
            --hidden-import regex \
            --hidden-import pynvml \
            --hidden-import lxml \
            --hidden-import lxml.etree \
            --hidden-import ebooklib \
            --hidden-import ebooklib.epub \
            --collect-all opencc \
            --add-data "murasaki_translator:murasaki_translator" \
            --add-data "rule_processor.py:." \
            murasaki_translator/main.py
          
          # ç§»åŠ¨åˆ° bin ç›®å½•ä¾› Electron ä½¿ç”¨
          mkdir -p bin/python-bundle
          mv dist/murasaki-engine bin/python-bundle/
          chmod +x bin/python-bundle/murasaki-engine
          
          echo "Built Python bundle:"
          ls -la bin/python-bundle/

      - name: Install Node dependencies
        working-directory: GUI
        run: npm install

      - name: Prepare packaged docs
        run: |
          mkdir -p release
          cp .github/release-docs/README-linux-gui.md release/readme.md
          cp LICENSE "release/LICENSE.Murasaki Translator.txt"

      - name: Build GUI
        working-directory: GUI
        run: |
          npm run build
          npx electron-builder --linux --x64
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Package CLI Server
        run: |
          mkdir -p dist-cli/murasaki-server
          mkdir -p dist-cli/murasaki-server/bin
          cp -r middleware/bin/linux-vulkan dist-cli/murasaki-server/bin/
          if [ -d middleware/bin/linux-cpu ]; then
            cp -r middleware/bin/linux-cpu dist-cli/murasaki-server/bin/
          fi
          if [ -d middleware/bin/linux-cuda ]; then
            cp -r middleware/bin/linux-cuda dist-cli/murasaki-server/bin/
          fi
          cp -r middleware/murasaki_translator dist-cli/murasaki-server/
          cp -r middleware/openai_proxy dist-cli/murasaki-server/
          cp -r middleware/server dist-cli/murasaki-server/
          cp middleware/cli/murasaki_server.py dist-cli/murasaki-server/
          cp middleware/requirements.txt dist-cli/murasaki-server/
          cp middleware/rule_processor.py dist-cli/murasaki-server/
          mkdir -p dist-cli/murasaki-server/glossaries

          # Merge requirements
          echo "" >> dist-cli/murasaki-server/requirements.txt
          cat middleware/openai_proxy/requirements.txt >> dist-cli/murasaki-server/requirements.txt
          echo "" >> dist-cli/murasaki-server/requirements.txt
          cat middleware/server/requirements.txt >> dist-cli/murasaki-server/requirements.txt

          # Create startup script
          cat > dist-cli/murasaki-server/start.sh << 'EOF'
          #!/bin/bash
          set -euo pipefail
          cd "$(dirname "$0")"
          exec bash server/start_server.sh "$@"
          EOF
          chmod +x dist-cli/murasaki-server/start.sh
          chmod +x dist-cli/murasaki-server/bin/linux-vulkan/llama-server
          if [ -f dist-cli/murasaki-server/bin/linux-cpu/llama-server ]; then
            chmod +x dist-cli/murasaki-server/bin/linux-cpu/llama-server
          fi
          if [ -f dist-cli/murasaki-server/bin/linux-cuda/llama-server ]; then
            chmod +x dist-cli/murasaki-server/bin/linux-cuda/llama-server
          fi

          # Add README and LICENSE
          cp .github/release-docs/README-linux-cli.md dist-cli/murasaki-server/README.md
          cp .github/release-docs/install-linux-server.sh dist-cli/murasaki-server/install.sh
          cp LICENSE dist-cli/murasaki-server/murasaki-translator.LICENSE.txt
          chmod +x dist-cli/murasaki-server/install.sh

          # Package archive
          cd dist-cli
          tar -czvf murasaki-server-linux-x64.tar.gz murasaki-server

      - name: Smoke test Linux server package
        run: |
          cd dist-cli
          tar -xzf murasaki-server-linux-x64.tar.gz
          cd murasaki-server
          python3 -m venv .venv
          .venv/bin/python -m pip install --upgrade pip
          .venv/bin/python -m pip install -r requirements.txt
          SMOKE_KEY="smoke-test-key"
          OPENAI_PROXY_TIMEOUT=60 ./start.sh --host 127.0.0.1 --port 18080 --api-key "$SMOKE_KEY" --enable-openai-proxy --openai-port 18081 > smoke.log 2>&1 &
          SERVER_PID=$!
          cleanup() {
            kill "$SERVER_PID" >/dev/null 2>&1 || true
          }
          trap cleanup EXIT

          READY=0
          for i in $(seq 1 300); do
            if curl -fsS http://127.0.0.1:18080/health >/tmp/health.json; then
              API_AUTH_CODE=$(curl -sS -o /tmp/status.json -w "%{http_code}" -H "Authorization: Bearer $SMOKE_KEY" http://127.0.0.1:18080/api/v1/status || true)
              OPENAI_AUTH_CODE=$(curl -sS -o /tmp/models.json -w "%{http_code}" -H "Authorization: Bearer $SMOKE_KEY" http://127.0.0.1:18081/v1/models || true)
              if [ "$API_AUTH_CODE" = "200" ] && [ "$OPENAI_AUTH_CODE" = "200" ]; then
                READY=1
                break
              fi
            fi
            sleep 1
          done

          if [ "$READY" -ne 1 ]; then
            echo "Server did not become ready in time"
            tail -n 200 smoke.log || true
            if [ -f openai-proxy.log ]; then
              echo "----- openai-proxy.log (tail) -----"
              tail -n 200 openai-proxy.log || true
              echo "-----------------------------------"
            fi
            exit 1
          fi

          HEALTH_CODE=$(curl -sS -o /tmp/health.json -w "%{http_code}" http://127.0.0.1:18080/health)
          API_AUTH_CODE=$(curl -sS -o /tmp/status.json -w "%{http_code}" -H "Authorization: Bearer $SMOKE_KEY" http://127.0.0.1:18080/api/v1/status)
          OPENAI_AUTH_CODE=$(curl -sS -o /tmp/models.json -w "%{http_code}" -H "Authorization: Bearer $SMOKE_KEY" http://127.0.0.1:18081/v1/models)
          API_NOAUTH_CODE=$(curl -sS -o /tmp/status_noauth.json -w "%{http_code}" http://127.0.0.1:18080/api/v1/status || true)
          OPENAI_NOAUTH_CODE=$(curl -sS -o /tmp/models_noauth.json -w "%{http_code}" http://127.0.0.1:18081/v1/models || true)

          test "$HEALTH_CODE" = "200"
          test "$API_AUTH_CODE" = "200"
          test "$OPENAI_AUTH_CODE" = "200"
          test "$API_NOAUTH_CODE" = "403"
          test "$OPENAI_NOAUTH_CODE" = "403"

          grep -q '"auth_required"[[:space:]]*:[[:space:]]*true' /tmp/health.json
          grep -q "api_v1" /tmp/health.json
          grep -q "api_v1_full_parity" /tmp/health.json
          grep -q "openai_v1" /tmp/health.json

          # Upload contract test
          echo "remote smoke payload" > /tmp/smoke_upload.txt
          UPLOAD_CODE=$(curl -sS -o /tmp/upload.json -w "%{http_code}" \
            -H "Authorization: Bearer $SMOKE_KEY" \
            -F "file=@/tmp/smoke_upload.txt;type=text/plain" \
            http://127.0.0.1:18080/api/v1/upload/file)
          test "$UPLOAD_CODE" = "200"
          python3 -c "import json; payload=json.load(open('/tmp/upload.json', encoding='utf-8')); assert payload.get('file_path'), 'upload file_path missing'"

          # Create/get/cancel task contract test (no model required for endpoint contract)
          TRANSLATE_CODE=$(curl -sS -o /tmp/translate.json -w "%{http_code}" \
            -H "Authorization: Bearer $SMOKE_KEY" \
            -H "Content-Type: application/json" \
            -d '{"text":"smoke translate contract"}' \
            http://127.0.0.1:18080/api/v1/translate)
          test "$TRANSLATE_CODE" = "200"
          python3 -c "import json; payload=json.load(open('/tmp/translate.json', encoding='utf-8')); task_id=payload.get('task_id'); assert task_id, 'task_id missing'; open('/tmp/task_id.txt', 'w', encoding='utf-8').write(task_id)"
          TASK_ID=$(cat /tmp/task_id.txt)

          TASK_STATUS_CODE=$(curl -sS -o /tmp/task_status.json -w "%{http_code}" \
            -H "Authorization: Bearer $SMOKE_KEY" \
            "http://127.0.0.1:18080/api/v1/translate/${TASK_ID}?log_from=0&log_limit=5")
          test "$TASK_STATUS_CODE" = "200"
          python3 -c "import json; payload=json.load(open('/tmp/task_status.json', encoding='utf-8')); required=['task_id','status','next_log_index','log_total','logs_truncated']; missing=[k for k in required if k not in payload]; assert not missing, f'missing field: {missing}'"

          # WebSocket contract test (use python websockets to avoid Node global WebSocket issues)
          .venv/bin/python - <<'PY'
          import asyncio
          import json
          import sys
          import websockets

          task_id = open('/tmp/task_id.txt', encoding='utf-8').read().strip()
          url = f"ws://127.0.0.1:18080/api/v1/ws/{task_id}?token=smoke-test-key"

          async def main():
              try:
                  async with websockets.connect(url, open_timeout=5) as ws:
                      msg = await asyncio.wait_for(ws.recv(), timeout=10)
                      payload = json.loads(msg)
                      if not isinstance(payload, dict) or "type" not in payload:
                          raise RuntimeError("websocket payload missing type")
              except Exception as exc:
                  print(exc)
                  sys.exit(1)

          asyncio.run(main())
          PY

          CANCEL_CODE=$(curl -sS -o /tmp/cancel_task.json -w "%{http_code}" \
            -X DELETE \
            -H "Authorization: Bearer $SMOKE_KEY" \
            "http://127.0.0.1:18080/api/v1/translate/${TASK_ID}")
          test "$CANCEL_CODE" = "200"

          DOWNLOAD_CODE=$(curl -sS -o /tmp/download_pending.bin -w "%{http_code}" \
            -H "Authorization: Bearer $SMOKE_KEY" \
            "http://127.0.0.1:18080/api/v1/download/${TASK_ID}" || true)
          test "$DOWNLOAD_CODE" = "400"
      - name: Upload GUI Artifact
        uses: actions/upload-artifact@v4
        with:
          name: Murasaki-linux-x64
          path: GUI/dist/*.AppImage
          retention-days: 7

      - name: Upload CLI Artifact
        uses: actions/upload-artifact@v4
        with:
          name: Murasaki-server-linux-x64
          path: dist-cli/*.tar.gz

  # ============================================
  # Create GitHub Release
  # ============================================
  release:
    needs: [build-win-cuda, build-win-vulkan, build-mac, build-linux]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/')
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Prepare release assets
        run: |
          mkdir -p release-assets
          # åªå¤åˆ¶éœ€è¦çš„æ–‡ä»¶æ ¼å¼
          find artifacts -type f \( -name "*.dmg" -o -name "*.zip" -o -name "*.AppImage" -o -name "*.tar.gz" \) -exec cp {} release-assets/ \;
          
          echo "Release assets:"
          ls -la release-assets/

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v2
        with:
          files: release-assets/*
          draft: false
          prerelease: ${{ contains(github.ref_name, 'beta') || contains(github.ref_name, 'alpha') }}
          generate_release_notes: true
          body: |
            ## ðŸ“¦ ä¸‹è½½è¯´æ˜Ž
            
            | å¹³å° | GPU | æ–‡ä»¶ |
            |------|-----|------|
            | Windows | NVIDIA (CUDA) | `*-win-cuda-x64.zip` |
            | Windows | AMD/Intel (Vulkan) | `*-win-vulkan-x64.zip` |
            | macOS | Apple Silicon | `*-arm64.dmg` |
            | macOS | Intel | `*.dmg` (æ—  arm64 åŽç¼€) |
            | Linux | GUI (Vulkan) | `*.AppImage` |
            | Linux | CLI æœåŠ¡å™¨ | `murasaki-server-*.tar.gz` |
            
            ## ðŸ–¥ï¸ Linux CLI æœåŠ¡å™¨
            
            ç”¨äºŽæ— ç•Œé¢æœåŠ¡å™¨éƒ¨ç½²ï¼Œæä¾› OpenAI å…¼å®¹ APIï¼š
            ```bash
            MODEL='/path/to/model.gguf'; API_KEY='replace-with-strong-key'; curl -fsSL https://github.com/soundstarrain/Murasaki-Translator/releases/latest/download/murasaki-server-linux-x64.tar.gz | tar -xz && cd murasaki-server && nohup ./start.sh --host 0.0.0.0 --port 8000 --model "$MODEL" --api-key "$API_KEY" --enable-openai-proxy --openai-port 8001 > server.log 2>&1 &
            ```
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
